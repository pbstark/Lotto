{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plausability of Lottery Luck\n",
    "\n",
    "#### [Dylan D. Daniels](http://statistics.berkeley.edu/people/dylan-david-daniels) and [Philip B. Stark](www.stat.berkeley.edu/~stark), Department of Statistics, University of California, Berkeley\n",
    "#### Based on MATLAB code by [Skip Garibaldi](http://www.garibaldibros.com/)\n",
    "\n",
    "This tool appraises whether it is plausible that a given individual won a set of lottery prizes honestly. \n",
    "\n",
    "This version uses data from a cell; other versions read data from .csv files.\n",
    "\n",
    "The user inputs an upper bound on the number of players (for instance, one might assume that the number of people playing the lottery isn't greater than the number of residents of the state), and a tiny \"threshold\" probability.\n",
    "\n",
    "The code outputs a lower bound on the amount all those people would have had to spend for any of them to have a tiny chance of winning so often, where \"tiny\" is the threshold number chosen by the user.\n",
    "\n",
    "If the required spending amount is, for example, several times the median house price in the state, it may call into question whether the winner won honestly.\n",
    "\n",
    "The current version can analyze data for only one gambler at a time. \n",
    "\n",
    "The code implements the mathematics described in the first link below. The third link is to a public lecture about the method, and results for reported lottery winners in Florida. \n",
    "The fourth and fifth links are news stories that relied on such calculations.\n",
    "\n",
    "See:\n",
    "+ Arratia, R., S. Garibaldi, L. Mower, and P.B. Stark, 2015. Some people have all the luck. _Mathematics Magazine_, _88_ 196–211. doi:10.4169/math.mag.88.3.196.c, Reprint: http://www.stat.berkeley.edu/~stark/Preprints/luck15.pdf http://www.jstor.org/stable/10.4169/math.mag.88.3.196\n",
    "+ Arratia, R., S. Garibaldi, L. Mower, and P.B. Stark, 2015. Some people have all the luck &hellip; or do they? _MAA Focus_, August/September, 37–38. http://www.maa.org/sites/default/files/pdf/MAAFocus/Focus_AugustSeptember_2015.pdf\n",
    "+ https://www.youtube.com/watch?v=s8cHHWNblA4\n",
    "+  Lottery odds: To win, you’d have to be a loser. Lawrence Mower, _Palm Beach Post_, 28 March 2014. http://www.mypalmbeachpost.com/news/news/lottery-odds-to-win-youd-have-to-be-a-loser/nfL57\n",
    "+ Against all Odds, Gavin Off and Adam Bell, _The Charlotte Observer_, 29 September 2016.\n",
    "http://www.charlotteobserver.com/news/special-reports/against-all-odds/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions:\n",
    "1. In the cell below, fill in data for the wins: p, n, and c\n",
    "\n",
    "Each entry corresponds to one type of wager. \"p\" is the chance of winning that wager; \"n\" is the number of times the gambler collected on that wager; and \"c\" is the cost per ticket or play on that wager. The computations assume that the gambler did not win any dependent bets, for instance, two bets on the same drawing.\n",
    "\n",
    "2. On the toolbar of this browser window (under the jupyter logo), click \"Cell\" --> \"Run All\". Wait a bit for your results to appear at the bottom of this page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = [ 1./5760,\\\n",
    "    1./4800\\\n",
    "    ]\n",
    "n = [ 370, 541+670+898+917+1121\\\n",
    "    ]\n",
    "c = [ 20,\\\n",
    "    20\\\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "# set the population size and overall cutoff probability\n",
    "POPULATION = 6.794e6   # population of MA in 2015\n",
    "THRESHOLD =  10**(-7) # one in ten million threshold\n",
    "CUT = THRESHOLD / POPULATION # Bonferroni cutoff probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def constraintFn(p, n): # constraint function: probability of vector of wins must be at least CUT\n",
    "    return lambda x: np.sum(binom.logsf(n-1, x, p)) - np.log(CUT)\n",
    "\n",
    "def objectiveFn(c):  # construct function that gives cost of vector x of bets, for cost-per-bet vector c\n",
    "    return lambda x: np.dot(x, c)\n",
    "\n",
    "def solve(x0, upperBoundVec, p, n, c, eps, debugMode, maxiter, method='SLSQP'):  \n",
    "    # invoke the constrained optimizer\n",
    "    # \n",
    "    #    x0:     starting guess\n",
    "    #    p:      vector of game probabilities\n",
    "    #    n:      vector of number of wins of each game\n",
    "    #    c:      vector of game costs\n",
    "    #    eps:    stepsize for Hessian approximation\n",
    "    #    debugMode: True for verbose output\n",
    "    #    maxiter: maximum iterations in optimizer\n",
    "    #    method: underlying minimization algorithm\n",
    "    #       \n",
    "    cons = ({'type': 'ineq', 'fun': constraintFn(p, n)})   # overall probability constraint\n",
    "    bnds = tuple((n[i], upperBoundVec[i]) for i in range(len(n)))  # must bet at least n times to win n times\n",
    "    return minimize(objectiveFn(c), x0, method=method, jac=(lambda x: c),\n",
    "                    constraints=cons, bounds=bnds,\n",
    "                    options={'disp': debugMode, 'maxiter': maxiter, 'eps': eps})\n",
    "\n",
    "def solveProblem(tries=5, debugMode=False, epsilon = 1e-7, epsFac=8, maxiter=10**4):\n",
    "    # Try up to epsFac values of the Hessian step size, related by powers of 10 (Hessian approximation step sizes)\n",
    "    optimalValues = []     # candidate optima\n",
    "    optimalProbs = []      # probabilities associated with those optima\n",
    "    optimalSolutions = []  # detailed optimization output for candidate optima\n",
    "    if debugMode:\n",
    "        print(\"n: {} \\np: {} \\nc:\".format(n,p,c))\n",
    "    for meth in methods:   # try different optimization methods\n",
    "        for epsIndex in range(epsFac):  # try different step sizes in the Hessian\n",
    "            x0 = np.array(that/divisor) # starting guess\n",
    "            for i in range(tries):\n",
    "                while (np.sum(binom.logsf(n-1, x0, p)) - np.log(CUT)) < 0:  # ensure x0 is a feasible point\n",
    "                    x0 = np.add(x0,np.ones_like(x0))  # increment every element of x\n",
    "                if (debugMode):\n",
    "                    print(\"method: {} try: {} \\nx0: {} \\nprobability {}:\".format(meth,i,x0,\\\n",
    "                                np.exp(np.sum(binom.logsf(n-1, x0, p)))))\n",
    "                optimOutput = solve(x0, that, p, n-1, c, epsilon*10**epsIndex, debugMode, maxiter, method=meth)\n",
    "                if optimOutput['success']:\n",
    "                    optimalValues.append(optimOutput['fun'])\n",
    "                    attainedProb = np.exp(np.sum(binom.logsf(n-1, optimOutput['x'], p)))\n",
    "                    optimalProbs.append(attainedProb)\n",
    "                    optimalSolutions.append(optimOutput)\n",
    "                    if debugMode:\n",
    "                        print(optimOutput)\n",
    "                        print(\"attained probability: {}\".format(attainedProb))\n",
    "                x0 = [np.random.randint(low=n[i], high=that[i]) for i in range(len(n))] # update x0 randomly\n",
    "    if len(optimalValues) == 0:\n",
    "        raise Exception('No candidate optimal solution found.')\n",
    "    bestValue = np.min(optimalValues)\n",
    "    largestProb = np.max(tuple(optimalProbs))\n",
    "    if debugMode:\n",
    "        print(\"\\nFound {} candidate minima: {}\".format(len(optimalValues), optimalValues))\n",
    "        print(\"Best value: {}\".format(bestValue))\n",
    "    print(\"Everyone in the population of {} people would have to spend at least ${:,} to have probability {} that at least one would win these bets so often.\"\n",
    "          .format(POPULATION, np.int(bestValue), THRESHOLD))\n",
    "    return optimalValues, optimalProbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial t_hat: [  2131200.  19905600.] \n",
      "initial log probability: -1.36843585088 \n",
      "target log probability -31.8496460787\n",
      "adjusted t_hat: [  2131200.  19905600.] \n",
      "adjusted probability: 0.254504731737\n"
     ]
    }
   ],
   "source": [
    "p = np.array(p)\n",
    "n = np.array(n)\n",
    "c = np.array(c)\n",
    "\n",
    "that = n/p  # expected number of wagers on each bet required to win that bet n times\n",
    "\n",
    "debugMode = True  # verbose output if True; set to False for less output\n",
    "\n",
    "np.random.RandomState(seed=1234567890) # setting seed explicitly, for reproducibility\n",
    "\n",
    "if debugMode:\n",
    "    print (\"initial t_hat: {} \\ninitial log probability: {} \\ntarget log probability {}\".format(\\\n",
    "          that,\\\n",
    "          np.sum(binom.logsf(n-1, that, p)),\\\n",
    "          np.log(CUT)))\n",
    "\n",
    "# 'that' will be used as an upper bound; ensure that it's compatible with the probability constraint\n",
    "while np.sum(binom.logsf(n-1, that, p)) < np.log(CUT):\n",
    "    that = 2*that\n",
    "    if debugMode:\n",
    "        print(\"estimated trials: {} log(tailprob): {}\".format(that,np.sum(binom.logsf(n-1, that, p))))\n",
    "\n",
    "divisor = 5 # initial value for optimizer is expected number divided by divisor (modified to ensure feasibility)\n",
    "methods = ['SLSQP','COBYLA']  # COBYLA will ignore the individual bounds, but should honor the probability constraint\n",
    "\n",
    "if debugMode:\n",
    "    print (\"adjusted t_hat: {} \\nadjusted probability: {}\".format(that,np.exp(np.sum(binom.logsf(n-1, that, p)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: [ 370 4147] \n",
      "p: [ 0.00017361  0.00020833] \n",
      "c:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/scipy/stats/_distn_infrastructure.py:815: RuntimeWarning: divide by zero encountered in log\n",
      "  return log(self._sf(x, *args))\n"
     ]
    }
   ],
   "source": [
    "optimalValues, optimalProbs = solveProblem(tries = 5, debugMode=debugMode, epsilon = 1e-7, epsFac=8, maxiter=10**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version information\n",
    "%load_ext version_information\n",
    "%version_information scipy, numpy, pandas, matplotlib, notebook"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
