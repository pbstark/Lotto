{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plausability of Lottery Luck\n",
    "\n",
    "#### [Dylan D. Daniels](http://statistics.berkeley.edu/people/dylan-david-daniels) and [Philip B. Stark](www.stat.berkeley.edu/~stark), Department of Statistics, University of California, Berkeley\n",
    "#### Based on MATLAB code by [Skip Garibaldi](http://www.garibaldibros.com/)\n",
    "\n",
    "This tool appraises whether it is plausible that a given individual won a set of lottery prizes honestly. \n",
    "\n",
    "The code reads a comma-separated values file (CSV) of wins and odds.\n",
    "\n",
    "The user inputs the number of residents of the state, and a tiny \"threshold\" probability.\n",
    "\n",
    "The code outputs a lower bound on the amount everyone in the state would have had to spend for any of them to have a tiny chance of winning so often, where \"tiny\" is the threshold number chosen by the user.\n",
    "\n",
    "If the required spending amount is, for example, several times the median house price in the state, it may call into question whether the winner won honestly.\n",
    "\n",
    "The current version can analyze data for only one gambler at a time. \n",
    "\n",
    "The code implements the mathematics described in the first link below. The third link is to a public lecture about the method, and results for reported lottery winners in Florida. \n",
    "\n",
    "See:\n",
    "+ Arratia, R., S. Garibaldi, L. Mower, and P.B. Stark, 2015. Some people have all the luck. _Mathematics Magazine_, _88_ 196–211. doi:10.4169/math.mag.88.3.196.c, Reprint: http://www.stat.berkeley.edu/~stark/Preprints/luck15.pdf http://www.jstor.org/stable/10.4169/math.mag.88.3.196\n",
    "+ Arratia, R., S. Garibaldi, L. Mower, and P.B. Stark, 2015. Some people have all the luck &hellip; or do they? _MAA Focus_, August/September, 37–38. http://www.maa.org/sites/default/files/pdf/MAAFocus/Focus_AugustSeptember_2015.pdf\n",
    "+ https://www.youtube.com/watch?v=s8cHHWNblA4\n",
    "+  Lottery odds: To win, you’d have to be a loser. Lawrence Mower, _Palm Beach Post_, 28 March 2014. http://www.mypalmbeachpost.com/news/news/lottery-odds-to-win-youd-have-to-be-a-loser/nfL57\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions:\n",
    "1. Compile a CSV file for each gambler. The CSV file should contain three columns:  \"Probability,\" \"Number,\" and \"Cost.\" \n",
    "\n",
    "Each row corresponds to one type of wager. \"Probability\" is the chance of winning that wager; \"Number\" is the number of times the gambler collected on that wager; and \"Cost\" is the cost per ticket or play on that wager.\n",
    "\n",
    "2. Put the filename of your CSV file in the box below, along with the values of POPULATION and THRESHOLD.\n",
    "\n",
    "3. On the toolbar of this browser window (under the jupyter logo), click \"Cell\" --> \"Run All\". Wait a bit for your results to appear at the bottom of this page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "# Put the name of your CSV file here:\n",
    "# CSV_FILENAME = 'FILL_ME_IN.csv'\n",
    "CSV_FILENAME = 'manning-edit.csv'\n",
    "# CSV_FILENAME = 'pandya.csv'\n",
    "\n",
    "# set the cutoff probability\n",
    "POPULATION = 5 # 10**7   # population of North Carolina\n",
    "THRESHOLD =  0.05 # 10**(-7) # one in ten million threshold\n",
    "\n",
    "CUT = THRESHOLD / POPULATION # Bonferroni cutoff probability\n",
    "\n",
    "debugMode = True\n",
    "print(CUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import betainc\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def binTail(p, n, t):\n",
    "    return betainc(n, t - n + 1, p)\n",
    "\n",
    "def constraintFn(p, n):\n",
    "    return lambda x: np.sum(np.log(binTail(p, n, x))) - np.log(CUT)\n",
    "\n",
    "def objectiveFn(c):\n",
    "    return lambda x: np.dot(x, c)\n",
    "\n",
    "def solve(x0, upperBoundVec, p, n, c, eps, debugMode, maxiter):\n",
    "    cons = ({'type': 'ineq', 'fun': constraintFn(p, n)})\n",
    "    bnds = tuple((n[i], upperBoundVec[i]) for i in range(len(n)))\n",
    "    return minimize(objectiveFn(c), x0, method='SLSQP', jac=(lambda x: c),\n",
    "                    constraints=cons, bounds=bnds,\n",
    "                    options={'disp': debugMode, 'maxiter': maxiter, 'eps': eps})\n",
    "\n",
    "def readCsv(filename):\n",
    "    with open(CSV_FILENAME, 'r') as f:\n",
    "        firstLine = f.readline()\n",
    "        if firstLine.strip() != \"Probability,Number,Cost\":\n",
    "            raise Exception('First line of CSV must be \"Probability,Number,Cost\"')\n",
    "    values = np.loadtxt(filename, dtype=np.float_, delimiter=',', skiprows=1)\n",
    "    values = np.atleast_2d(values)\n",
    "    pValues = values[:,0]\n",
    "    nValues = values[:,1]\n",
    "    cValues = values[:,2]\n",
    "    return (pValues, nValues, cValues)\n",
    "\n",
    "def calculateBound(eps, debugMode, maxiter):\n",
    "    (p, n, c) = readCsv(CSV_FILENAME)\n",
    "    that = n / p\n",
    "    x0 = that / 4\n",
    "    return solve(x0, that, p, n, c, eps, debugMode, maxiter)\n",
    "\n",
    "def solveProblem(tries=3, debugMode=False, epsilon = 1e-6, epsFac=5, maxiter=10**4):\n",
    "    # Try up to epsFac values of the Hessian step size, related by powers of 10 (Hessian approximation step sizes)\n",
    "    epsIndex = 0\n",
    "    optimalValues = []\n",
    "    i = 0\n",
    "    for epsIndex in range(epsFac):\n",
    "        for i in range(tries):\n",
    "            optimOutput = calculateBound(epsilon*10**epsIndex, debugMode, maxiter)\n",
    "            if optimOutput['status'] == 0:\n",
    "                optimalValues.append(optimOutput['fun'])\n",
    "                print(optimOutput)\n",
    "    if len(optimalValues) == 0:\n",
    "        raise Exception('Something went wrong while solving the problem. Please ask for assistance.')\n",
    "    bestValue = np.min(optimalValues)\n",
    "    if debugMode:\n",
    "        print(\"Found {} local minima: {}\".format(len(optimalValues), optimalValues))\n",
    "    print(\"Everyone in the population would have to spend at least ${:,} dollars to have probability {} that at least one would win so much.\"\n",
    "          .format(np.int(bestValue),THRESHOLD))\n",
    "    return bestValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inequality constraints incompatible    (Exit mode 4)\n",
      "            Current function value: 2414017.5508\n",
      "            Iterations: 81\n",
      "            Function evaluations: 708\n",
      "            Gradient evaluations: 78\n",
      "Inequality constraints incompatible    (Exit mode 4)\n",
      "            Current function value: 2414017.5508\n",
      "            Iterations: 81\n",
      "            Function evaluations: 708\n",
      "            Gradient evaluations: 78\n",
      "Inequality constraints incompatible    (Exit mode 4)\n",
      "            Current function value: 2414017.5508\n",
      "            Iterations: 81\n",
      "            Function evaluations: 708\n",
      "            Gradient evaluations: 78\n",
      "Inequality constraints incompatible    (Exit mode 4)\n",
      "            Current function value: 2414017.5508\n",
      "            Iterations: 81\n",
      "            Function evaluations: 708\n",
      "            Gradient evaluations: 78\n",
      "Inequality constraints incompatible    (Exit mode 4)\n",
      "            Current function value: 2414017.5508\n",
      "            Iterations: 81\n",
      "            Function evaluations: 708\n",
      "            Gradient evaluations: 78\n",
      "Positive directional derivative for linesearch    (Exit mode 8)\n",
      "            Current function value: 2239057.08743\n",
      "            Iterations: 205\n",
      "            Function evaluations: 922\n",
      "            Gradient evaluations: 201\n",
      "Positive directional derivative for linesearch    (Exit mode 8)\n",
      "            Current function value: 2239057.08743\n",
      "            Iterations: 205\n",
      "            Function evaluations: 922\n",
      "            Gradient evaluations: 201\n",
      "Positive directional derivative for linesearch    (Exit mode 8)\n",
      "            Current function value: 2239057.08743\n",
      "            Iterations: 205\n",
      "            Function evaluations: 922\n",
      "            Gradient evaluations: 201\n",
      "Positive directional derivative for linesearch    (Exit mode 8)\n",
      "            Current function value: 2239057.08743\n",
      "            Iterations: 205\n",
      "            Function evaluations: 922\n",
      "            Gradient evaluations: 201\n",
      "Positive directional derivative for linesearch    (Exit mode 8)\n",
      "            Current function value: 2239057.08743\n",
      "            Iterations: 205\n",
      "            Function evaluations: 922\n",
      "            Gradient evaluations: 201\n",
      "Positive directional derivative for linesearch    (Exit mode 8)\n",
      "            Current function value: 2408055.44911\n",
      "            Iterations: 94\n",
      "            Function evaluations: 698\n",
      "            Gradient evaluations: 90\n",
      "Positive directional derivative for linesearch    (Exit mode 8)\n",
      "            Current function value: 2408055.44911\n",
      "            Iterations: 94\n",
      "            Function evaluations: 698\n",
      "            Gradient evaluations: 90\n",
      "Positive directional derivative for linesearch    (Exit mode 8)\n",
      "            Current function value: 2408055.44911\n",
      "            Iterations: 94\n",
      "            Function evaluations: 698\n",
      "            Gradient evaluations: 90\n",
      "Positive directional derivative for linesearch    (Exit mode 8)\n",
      "            Current function value: 2408055.44911\n",
      "            Iterations: 94\n",
      "            Function evaluations: 698\n",
      "            Gradient evaluations: 90\n",
      "Positive directional derivative for linesearch    (Exit mode 8)\n",
      "            Current function value: 2408055.44911\n",
      "            Iterations: 94\n",
      "            Function evaluations: 698\n",
      "            Gradient evaluations: 90\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 2412730.2315\n",
      "            Iterations: 47\n",
      "            Function evaluations: 269\n",
      "            Gradient evaluations: 43\n",
      "  status: 0\n",
      " success: True\n",
      "    njev: 43\n",
      "    nfev: 269\n",
      "     fun: 2412730.2314952575\n",
      "       x: array([   3379.99986479,   17099.96067009,    3076.00027684,\n",
      "         10000.        ,    7058.0097824 ,   14994.01310477,\n",
      "         12000.        ,    8116.02671796,    2399.99808   ,\n",
      "          5999.98800002,   69683.96689127,    4800.00768001,\n",
      "         14999.9925    ,   20000.        ,   39999.99999999,\n",
      "         20000.        ,  120000.04799948,   14400.02304004,\n",
      "          2222.00002222])\n",
      " message: 'Optimization terminated successfully.'\n",
      "     jac: array([ 10.,  10.,  20.,  20.,   5.,  20.,  10.,  10.,  20.,  10.,   5.,\n",
      "        10.,   5.,   5.,   2.,  10.,   2.,  10.,  30.,   0.])\n",
      "     nit: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 2412730.2315\n",
      "            Iterations: 47\n",
      "            Function evaluations: 269\n",
      "            Gradient evaluations: 43\n",
      "  status: 0\n",
      " success: True\n",
      "    njev: 43\n",
      "    nfev: 269\n",
      "     fun: 2412730.2314952575\n",
      "       x: array([   3379.99986479,   17099.96067009,    3076.00027684,\n",
      "         10000.        ,    7058.0097824 ,   14994.01310477,\n",
      "         12000.        ,    8116.02671796,    2399.99808   ,\n",
      "          5999.98800002,   69683.96689127,    4800.00768001,\n",
      "         14999.9925    ,   20000.        ,   39999.99999999,\n",
      "         20000.        ,  120000.04799948,   14400.02304004,\n",
      "          2222.00002222])\n",
      " message: 'Optimization terminated successfully.'\n",
      "     jac: array([ 10.,  10.,  20.,  20.,   5.,  20.,  10.,  10.,  20.,  10.,   5.,\n",
      "        10.,   5.,   5.,   2.,  10.,   2.,  10.,  30.,   0.])\n",
      "     nit: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 2412730.2315\n",
      "            Iterations: 47\n",
      "            Function evaluations: 269\n",
      "            Gradient evaluations: 43\n",
      "  status: 0\n",
      " success: True\n",
      "    njev: 43\n",
      "    nfev: 269\n",
      "     fun: 2412730.2314952575\n",
      "       x: array([   3379.99986479,   17099.96067009,    3076.00027684,\n",
      "         10000.        ,    7058.0097824 ,   14994.01310477,\n",
      "         12000.        ,    8116.02671796,    2399.99808   ,\n",
      "          5999.98800002,   69683.96689127,    4800.00768001,\n",
      "         14999.9925    ,   20000.        ,   39999.99999999,\n",
      "         20000.        ,  120000.04799948,   14400.02304004,\n",
      "          2222.00002222])\n",
      " message: 'Optimization terminated successfully.'\n",
      "     jac: array([ 10.,  10.,  20.,  20.,   5.,  20.,  10.,  10.,  20.,  10.,   5.,\n",
      "        10.,   5.,   5.,   2.,  10.,   2.,  10.,  30.,   0.])\n",
      "     nit: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 2412730.2315\n",
      "            Iterations: 47\n",
      "            Function evaluations: 269\n",
      "            Gradient evaluations: 43\n",
      "  status: 0\n",
      " success: True\n",
      "    njev: 43\n",
      "    nfev: 269\n",
      "     fun: 2412730.2314952575\n",
      "       x: array([   3379.99986479,   17099.96067009,    3076.00027684,\n",
      "         10000.        ,    7058.0097824 ,   14994.01310477,\n",
      "         12000.        ,    8116.02671796,    2399.99808   ,\n",
      "          5999.98800002,   69683.96689127,    4800.00768001,\n",
      "         14999.9925    ,   20000.        ,   39999.99999999,\n",
      "         20000.        ,  120000.04799948,   14400.02304004,\n",
      "          2222.00002222])\n",
      " message: 'Optimization terminated successfully.'\n",
      "     jac: array([ 10.,  10.,  20.,  20.,   5.,  20.,  10.,  10.,  20.,  10.,   5.,\n",
      "        10.,   5.,   5.,   2.,  10.,   2.,  10.,  30.,   0.])\n",
      "     nit: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 2412730.2315\n",
      "            Iterations: 47\n",
      "            Function evaluations: 269\n",
      "            Gradient evaluations: 43\n",
      "  status: 0\n",
      " success: True\n",
      "    njev: 43\n",
      "    nfev: 269\n",
      "     fun: 2412730.2314952575\n",
      "       x: array([   3379.99986479,   17099.96067009,    3076.00027684,\n",
      "         10000.        ,    7058.0097824 ,   14994.01310477,\n",
      "         12000.        ,    8116.02671796,    2399.99808   ,\n",
      "          5999.98800002,   69683.96689127,    4800.00768001,\n",
      "         14999.9925    ,   20000.        ,   39999.99999999,\n",
      "         20000.        ,  120000.04799948,   14400.02304004,\n",
      "          2222.00002222])\n",
      " message: 'Optimization terminated successfully.'\n",
      "     jac: array([ 10.,  10.,  20.,  20.,   5.,  20.,  10.,  10.,  20.,  10.,   5.,\n",
      "        10.,   5.,   5.,   2.,  10.,   2.,  10.,  30.,   0.])\n",
      "     nit: 47\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 2412581.75095\n",
      "            Iterations: 25\n",
      "            Function evaluations: 73\n",
      "            Gradient evaluations: 21\n",
      "  status: 0\n",
      " success: True\n",
      "    njev: 21\n",
      "    nfev: 73\n",
      "     fun: 2412581.7509473749\n",
      "       x: array([   3379.9998648 ,   17099.96067009,    3076.00027684,\n",
      "         10000.        ,    7058.0097824 ,   14994.01310477,\n",
      "         12000.        ,    8116.02671796,    2399.99808   ,\n",
      "          5999.98800002,   69654.27078146,    4800.00768001,\n",
      "         14999.9925    ,   20000.        ,   40000.        ,\n",
      "         20000.        ,  120000.04800002,   14400.02304004,\n",
      "          2222.00002222])\n",
      " message: 'Optimization terminated successfully.'\n",
      "     jac: array([ 10.,  10.,  20.,  20.,   5.,  20.,  10.,  10.,  20.,  10.,   5.,\n",
      "        10.,   5.,   5.,   2.,  10.,   2.,  10.,  30.,   0.])\n",
      "     nit: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 2412581.75095\n",
      "            Iterations: 25\n",
      "            Function evaluations: 73\n",
      "            Gradient evaluations: 21\n",
      "  status: 0\n",
      " success: True\n",
      "    njev: 21\n",
      "    nfev: 73\n",
      "     fun: 2412581.7509473749\n",
      "       x: array([   3379.9998648 ,   17099.96067009,    3076.00027684,\n",
      "         10000.        ,    7058.0097824 ,   14994.01310477,\n",
      "         12000.        ,    8116.02671796,    2399.99808   ,\n",
      "          5999.98800002,   69654.27078146,    4800.00768001,\n",
      "         14999.9925    ,   20000.        ,   40000.        ,\n",
      "         20000.        ,  120000.04800002,   14400.02304004,\n",
      "          2222.00002222])\n",
      " message: 'Optimization terminated successfully.'\n",
      "     jac: array([ 10.,  10.,  20.,  20.,   5.,  20.,  10.,  10.,  20.,  10.,   5.,\n",
      "        10.,   5.,   5.,   2.,  10.,   2.,  10.,  30.,   0.])\n",
      "     nit: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 2412581.75095\n",
      "            Iterations: 25\n",
      "            Function evaluations: 73\n",
      "            Gradient evaluations: 21\n",
      "  status: 0\n",
      " success: True\n",
      "    njev: 21\n",
      "    nfev: 73\n",
      "     fun: 2412581.7509473749\n",
      "       x: array([   3379.9998648 ,   17099.96067009,    3076.00027684,\n",
      "         10000.        ,    7058.0097824 ,   14994.01310477,\n",
      "         12000.        ,    8116.02671796,    2399.99808   ,\n",
      "          5999.98800002,   69654.27078146,    4800.00768001,\n",
      "         14999.9925    ,   20000.        ,   40000.        ,\n",
      "         20000.        ,  120000.04800002,   14400.02304004,\n",
      "          2222.00002222])\n",
      " message: 'Optimization terminated successfully.'\n",
      "     jac: array([ 10.,  10.,  20.,  20.,   5.,  20.,  10.,  10.,  20.,  10.,   5.,\n",
      "        10.,   5.,   5.,   2.,  10.,   2.,  10.,  30.,   0.])\n",
      "     nit: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 2412581.75095\n",
      "            Iterations: 25\n",
      "            Function evaluations: 73\n",
      "            Gradient evaluations: 21\n",
      "  status: 0\n",
      " success: True\n",
      "    njev: 21\n",
      "    nfev: 73\n",
      "     fun: 2412581.7509473749\n",
      "       x: array([   3379.9998648 ,   17099.96067009,    3076.00027684,\n",
      "         10000.        ,    7058.0097824 ,   14994.01310477,\n",
      "         12000.        ,    8116.02671796,    2399.99808   ,\n",
      "          5999.98800002,   69654.27078146,    4800.00768001,\n",
      "         14999.9925    ,   20000.        ,   40000.        ,\n",
      "         20000.        ,  120000.04800002,   14400.02304004,\n",
      "          2222.00002222])\n",
      " message: 'Optimization terminated successfully.'\n",
      "     jac: array([ 10.,  10.,  20.,  20.,   5.,  20.,  10.,  10.,  20.,  10.,   5.,\n",
      "        10.,   5.,   5.,   2.,  10.,   2.,  10.,  30.,   0.])\n",
      "     nit: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 2412581.75095\n",
      "            Iterations: 25\n",
      "            Function evaluations: 73\n",
      "            Gradient evaluations: 21\n",
      "  status: 0\n",
      " success: True\n",
      "    njev: 21\n",
      "    nfev: 73\n",
      "     fun: 2412581.7509473749\n",
      "       x: array([   3379.9998648 ,   17099.96067009,    3076.00027684,\n",
      "         10000.        ,    7058.0097824 ,   14994.01310477,\n",
      "         12000.        ,    8116.02671796,    2399.99808   ,\n",
      "          5999.98800002,   69654.27078146,    4800.00768001,\n",
      "         14999.9925    ,   20000.        ,   40000.        ,\n",
      "         20000.        ,  120000.04800002,   14400.02304004,\n",
      "          2222.00002222])\n",
      " message: 'Optimization terminated successfully.'\n",
      "     jac: array([ 10.,  10.,  20.,  20.,   5.,  20.,  10.,  10.,  20.,  10.,   5.,\n",
      "        10.,   5.,   5.,   2.,  10.,   2.,  10.,  30.,   0.])\n",
      "     nit: 25\n",
      "Found 10 local minima: [2412730.2314952575, 2412730.2314952575, 2412730.2314952575, 2412730.2314952575, 2412730.2314952575, 2412581.7509473749, 2412581.7509473749, 2412581.7509473749, 2412581.7509473749, 2412581.7509473749]\n",
      "Everyone in the population would have to spend at least $2,412,581 dollars to have probability 0.05 that at least one would win so much.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2412581.7509473749"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solveProblem(tries = 5, debugMode=debugMode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
